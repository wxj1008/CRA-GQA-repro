# <a href="https://arxiv.org/pdf/2503.07635"> Cross-modal Causal Relation Alignment for Video Question Grounding </a>

[Weixing Chen](https://wissingchen.github.io/), [Yang Liu](https://yangliu9208.github.io/), Binglin Chen, Jiandong Su, Yongsen Zheng, [Liang Lin](http://www.linliang.net/)

**Selected as CVPR 2025 Highlight! ğŸ˜ğŸ˜ğŸ˜**

<img src="method.png" align="center" />

Video question grounding (VideoQG) requires models to answer the questions and simultaneously infer the relevant video segments to support the answers. However, existing VideoQG methods usually suffer from spurious cross-modal correlations, leading to a failure to identify the dominant visual scenes that align with the intended question. Moreover, vision-language models exhibit unfaithful generalization performance and lack robustness on challenging downstream tasks such as VideoQG. In this work, we propose a novel VideoQG framework named Cross-modal Causal Relation Alignment (CRA), to eliminate spurious correlations and improve the causal consistency between question-answering and video temporal grounding. 

## Usage
  ### 1. Installation
  ```
  # clone the repository
  git clone https://github.com/WissingChen/CRA-GQA.git

  # create a conda environment
  conda env create -f requirements.yml
  ```

  ### 2. Datasets
  The CRA framework is evaluated on two VideoQG datasets: [NextGQA](https://github.com/doc-doc/NExT-GQA) and [STAR](https://bobbywu.com/STAR/). 

  #### prepare the data

  You can follow the preprocessing procedure mentioned in NextGQA to obtain the corresponding video features, QA annotations, and the required timestamps for evaluation.

  #### sample the multi-modal feature for Causal Internvention

  After preparing the above-mentioned data, you can proceed with further processing using the `.ipynb` file provided in the root directory.

  `sample_linguistic_feature.ipynb` -> semantic structure graph feature $\hat{L}$
  `sample_visual_feature.ipynb` -> video feature $\hat{V}$

  #### file structure
  ```bash
  data/
    â”œâ”€â”€ nextgqa
    â”‚   â”œâ”€â”€ causal_feature
    â”‚   â”œâ”€â”€ frame2time_test.json
    â”‚   â”œâ”€â”€ frame2time_val.json
    â”‚   â”œâ”€â”€ gsub_test.json
    â”‚   â”œâ”€â”€ gsub_val.json
    â”‚   â”œâ”€â”€ map_vid_vidorID.json
    â”‚   â”œâ”€â”€ test.csv
    â”‚   â”œâ”€â”€ train.csv
    â”‚   â”œâ”€â”€ train_gpt4_sub.json
    â”‚   â”œâ”€â”€ upbd_test.json
    â”‚   â”œâ”€â”€ upbd_val.json
    â”‚   â””â”€â”€ val.csv
    â”œâ”€â”€ nextqa
    â”‚   â”œâ”€â”€ frames
    â”‚   â”œâ”€â”€ test.csv
    â”‚   â”œâ”€â”€ train.csv
    â”‚   â”œâ”€â”€ val.csv
    â”‚   â””â”€â”€ video_feature
    â”œâ”€â”€ star
    â”‚   â”œâ”€â”€ causal_feature
    â”‚   â”œâ”€â”€ frame2time_test.json
    â”‚   â”œâ”€â”€ frame2time_train.json
    â”‚   â”œâ”€â”€ frame2time_val.json
    â”‚   â”œâ”€â”€ frames
    â”‚   â”œâ”€â”€ gsub_test.json
    â”‚   â”œâ”€â”€ gsub_train.json
    â”‚   â”œâ”€â”€ gsub_val.json
    â”‚   â”œâ”€â”€ split_file.json
    â”‚   â”œâ”€â”€ STAR_test.json
    â”‚   â”œâ”€â”€ STAR_train.json
    â”‚   â”œâ”€â”€ STAR_val.json
    â”‚   â”œâ”€â”€ test.csv
    â”‚   â”œâ”€â”€ train.csv
    â”‚   â”œâ”€â”€ val.csv
    â”‚   â”œâ”€â”€ video_feature
    â”‚   â””â”€â”€ Video_Segments.csv
  ```

  ### 3. Train
  Currently, I have only released the complete CRA code that uses **TempCLIP** as the backbone(however, I have included the core code and results in the **FrozenBiLM** folder).

  First, after preparing the data, you should modify the relevant parameters in the config folder, including the data path, etc.

  Then, you can simply run the `main.py` file directly.

  ### 4. Inference
  During the inference stage, don't forget to add the weight path for **resume** in the config, and then run `main.py --infer True`.

## One More Thing

The causal module used in the paper has been integrated into the open-source causal framework [CausalVLR](https://github.com/HCPLab-SYSU/CausalVLR). We welcome everyone to explore it and provide suggestions.

## Citation 
```
@inproceedings{chen2025cross,
  title={Cross-modal Causal Relation Alignment for Video Question Grounding},
  author={Chen, Weixing and Liu, Yang and Chen, Binglin and Su, Jiandong and Zheng, Yongsen and Lin, Liang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
```
